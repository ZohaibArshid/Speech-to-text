{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c02dba3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Model D:\\Forbmax User Data\\waqar sahi\\SpeechToText\\whisper-finetune-master\\output_model_dir not found; available models = ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mForbmax User Data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mwaqar sahi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSpeechToText\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mwhisper-finetune-master\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moutput_model_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\waqar_speech_finetune\\Lib\\site-packages\\whisper\\__init__.py:137\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[0;32m    135\u001b[0m     alignment_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found; available models = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_models()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[0;32m    142\u001b[0m     io\u001b[38;5;241m.\u001b[39mBytesIO(checkpoint_file) \u001b[38;5;28;01mif\u001b[39;00m in_memory \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(checkpoint_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    144\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(fp, map_location\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Model D:\\Forbmax User Data\\waqar sahi\\SpeechToText\\whisper-finetune-master\\output_model_dir not found; available models = ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large']"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(r\"D:\\Forbmax User Data\\waqar sahi\\SpeechToText\\whisper-finetune-master\\output_model_dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a923b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path=r\"D:\\a.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3329f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(audio_path)\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51bdbaa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Model D:\\Forbmax User Data\\waqar sahi\\SpeechToText\\whisper-finetune-master\\output_model_dir not found; available models = ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwhisper\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mForbmax User Data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mwaqar sahi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSpeechToText\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mwhisper-finetune-master\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moutput_model_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\waqar_speech_finetune\\Lib\\site-packages\\whisper\\__init__.py:137\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[0;32m    135\u001b[0m     alignment_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found; available models = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_models()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[0;32m    142\u001b[0m     io\u001b[38;5;241m.\u001b[39mBytesIO(checkpoint_file) \u001b[38;5;28;01mif\u001b[39;00m in_memory \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(checkpoint_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    144\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(fp, map_location\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Model D:\\Forbmax User Data\\waqar sahi\\SpeechToText\\whisper-finetune-master\\output_model_dir not found; available models = ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large-v1', 'large-v2', 'large']"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(r\"D:\\Forbmax User Data\\waqar sahi\\SpeechToText\\whisper-finetune-master\\output_model_dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(audio_path)\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9de33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "368d0caf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open file 'model.bin' in model 'D:\\Forbmax User Data\\waqar sahi\\SpeechToText\\whisper-finetune-master\\output_model_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m model_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mForbmax User Data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mwaqar sahi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSpeechToText\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mwhisper-finetune-master\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124moutput_model_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Run on GPU with FP16\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m WhisperModel(model_size, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, compute_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# or run on GPU with INT8\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# or run on CPU with INT8\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m segments, info \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtranscribe(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ma.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, beam_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[1;32md:\\forbmax user data\\waqar sahi\\stt\\faster-whisper\\faster_whisper\\transcribe.py:128\u001b[0m, in \u001b[0;36mWhisperModel.__init__\u001b[1;34m(self, model_size_or_path, device, device_index, compute_type, cpu_threads, num_workers, download_root, local_files_only)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m download_model(\n\u001b[0;32m    123\u001b[0m         model_size_or_path,\n\u001b[0;32m    124\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    125\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mdownload_root,\n\u001b[0;32m    126\u001b[0m     )\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m ctranslate2\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mWhisper(\n\u001b[0;32m    129\u001b[0m     model_path,\n\u001b[0;32m    130\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m    131\u001b[0m     device_index\u001b[38;5;241m=\u001b[39mdevice_index,\n\u001b[0;32m    132\u001b[0m     compute_type\u001b[38;5;241m=\u001b[39mcompute_type,\n\u001b[0;32m    133\u001b[0m     intra_threads\u001b[38;5;241m=\u001b[39mcpu_threads,\n\u001b[0;32m    134\u001b[0m     inter_threads\u001b[38;5;241m=\u001b[39mnum_workers,\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    137\u001b[0m tokenizer_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(tokenizer_file):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to open file 'model.bin' in model 'D:\\Forbmax User Data\\waqar sahi\\SpeechToText\\whisper-finetune-master\\output_model_dir'"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model_size = r\"D:\\Forbmax User Data\\waqar sahi\\SpeechToText\\whisper-finetune-master\\output_model_dir\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "\n",
    "# or run on GPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "# or run on CPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "segments, info = model.transcribe(r\"D:\\a.wav\", beam_size=5)\n",
    "\n",
    "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedb7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "650c05c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "temp_dir\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "temp_dir does not appear to have a file named config.json. Checkout 'https://huggingface.co/temp_dir/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m     model_id \u001b[38;5;241m=\u001b[39m hf_model\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_id)\n\u001b[1;32m---> 27\u001b[0m transcribe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m     28\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautomatic-speech-recognition\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[0;32m     30\u001b[0m     chunk_length_s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m     31\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m transcribe\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mforced_decoder_ids \u001b[38;5;241m=\u001b[39m transcribe\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mget_decoder_prompt_ids(language\u001b[38;5;241m=\u001b[39mlanguage, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\waqar_speech_finetune\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:695\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    693\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 695\u001b[0m     config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m    696\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39m_commit_hash\n\u001b[0;32m    698\u001b[0m custom_tasks \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\waqar_speech_finetune\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:916\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    914\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    915\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 916\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\waqar_speech_finetune\\Lib\\site-packages\\transformers\\configuration_utils.py:573\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    572\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 573\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    575\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\waqar_speech_finetune\\Lib\\site-packages\\transformers\\configuration_utils.py:628\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    626\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m    629\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    630\u001b[0m         configuration_file,\n\u001b[0;32m    631\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    632\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    633\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    634\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    635\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    636\u001b[0m         use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token,\n\u001b[0;32m    637\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    638\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    639\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m    640\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m    641\u001b[0m     )\n\u001b[0;32m    642\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    644\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\waqar_speech_finetune\\Lib\\site-packages\\transformers\\utils\\hub.py:380\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(resolved_file):\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[1;32m--> 380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have a file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    382\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m         )\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: temp_dir does not appear to have a file named config.json. Checkout 'https://huggingface.co/temp_dir/None' for available files."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your parameters here\n",
    "is_public_repo = False\n",
    "hf_model = \"openai/whisper-tiny\"\n",
    "ckpt_dir = r\"D:\\Forbmax User Data\\waqar sahi\\SpeechToText\\whisper-finetune-master\\output_model_dir\"\n",
    "temp_ckpt_folder = \"temp_dir\"\n",
    "path_to_audio = r\"D:\\a.wav\"\n",
    "language = \"ur\"\n",
    "device = 0  # Set to -1 for CPU, 0 for the first GPU, etc.\n",
    "\n",
    "if not is_public_repo:\n",
    "    print(\"here\")\n",
    "    os.system(f\"mkdir -p {temp_ckpt_folder}\")\n",
    "    ckpt_dir_parent = str(Path(ckpt_dir).parent)\n",
    "    os.system(f\"cp {ckpt_dir_parent}/added_tokens.json {ckpt_dir_parent}/normalizer.json \\\n",
    "    {ckpt_dir_parent}/preprocessor_config.json {ckpt_dir_parent}/special_tokens_map.json \\\n",
    "    {ckpt_dir_parent}/tokenizer_config.json {ckpt_dir_parent}/merges.txt \\\n",
    "    {ckpt_dir_parent}/vocab.json {ckpt_dir}/config.json {ckpt_dir}/pytorch_model.bin \\\n",
    "    {ckpt_dir}/training_args.bin {temp_ckpt_folder}\")\n",
    "    model_id = temp_ckpt_folder\n",
    "else:\n",
    "    model_id = hf_model\n",
    "print(model_id)\n",
    "transcribe = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=model_id,\n",
    "    chunk_length_s=30,\n",
    "    device=device,\n",
    ")\n",
    "print(\"hi\")\n",
    "transcribe.model.config.forced_decoder_ids = transcribe.tokenizer.get_decoder_prompt_ids(language=language, task=\"transcribe\")\n",
    "print('Transcription: ')\n",
    "print(transcribe(path_to_audio)[\"text\"])\n",
    "\n",
    "if not is_public_repo:\n",
    "    os.system(f\"rm -r {temp_ckpt_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dde334fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in d:\\programdata\\anaconda3\\envs\\waqar_speech_finetune\\lib\\site-packages (2.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd2aff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
